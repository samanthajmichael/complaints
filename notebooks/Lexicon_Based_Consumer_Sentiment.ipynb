{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sBwK6aS5BpZF"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install symspellpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OlLKxlmUBk4X"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#!pip uninstall -y nltk\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zH19J_0SMwQj",
    "outputId": "6585fa94-0cb6-4d23-df5c-9acb540f5b23"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install textblob\n",
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xqvXHzo6BX0D",
    "outputId": "f5e697d1-4b38-484e-ad43-ee6f41e6eaea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\saman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\saman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "from symspellpy import SymSpell\n",
    "import pkg_resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "gTXoAzdVBz50"
   },
   "outputs": [],
   "source": [
    "def load_github_data(url):\n",
    "    \"\"\"\n",
    "    Load data from GitHub raw content URL\n",
    "    Example URL: https://raw.githubusercontent.com/samanthajmichael/machine_learning/main/data/complaints.csv\n",
    "    \"\"\"\n",
    "    return pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "xQNi9_4HCIlY"
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/samanthajmichael/machine_learning/main/data/complaints.csv\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "OcauilSeB2wq"
   },
   "outputs": [],
   "source": [
    "df = df.copy(deep=True)\n",
    "df = df.rename(columns={'Date received': 'Date', 'Consumer complaint narrative': 'Complaint'})\n",
    "df = df.loc[(df['Product']=='Bank account or service') | \n",
    "(df['Product']=='Checking or savings account') | \n",
    "(df['Product']=='Money transfers') | \n",
    "(df['Product']=='Money transfer, virtual currency, or money service')]\n",
    "df=df[['Date', 'Product', 'Complaint']]\n",
    "df = df.set_index(pd.to_datetime(df['Date'], format='mixed'))\n",
    "df.drop(['Date'], axis=1, inplace=True)\n",
    "df = df.dropna(subset=['Complaint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "ga5cFopzCPDv",
    "outputId": "d558e371-184a-42fe-9ae5-edd330ff2908"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Complaint</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-10-26</th>\n",
       "      <td>Money transfer, virtual currency, or money ser...</td>\n",
       "      <td>On XX/XX/2022, I was contacted by XXXX XXXX ; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-10</th>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>I had been banking with Wells Fargo since XXXX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-31</th>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>From XXXX until XXXXXXXX XXXX XXXX someone had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-02</th>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>Several years ago opened an additional savings...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-01</th>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>I stopped using my wells Fargo account because...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Product  \\\n",
       "Date                                                            \n",
       "2022-10-26  Money transfer, virtual currency, or money ser...   \n",
       "2023-02-10                        Checking or savings account   \n",
       "2024-10-31                        Checking or savings account   \n",
       "2023-02-02                        Checking or savings account   \n",
       "2023-03-01                        Checking or savings account   \n",
       "\n",
       "                                                    Complaint  \n",
       "Date                                                           \n",
       "2022-10-26  On XX/XX/2022, I was contacted by XXXX XXXX ; ...  \n",
       "2023-02-10  I had been banking with Wells Fargo since XXXX...  \n",
       "2024-10-31  From XXXX until XXXXXXXX XXXX XXXX someone had...  \n",
       "2023-02-02  Several years ago opened an additional savings...  \n",
       "2023-03-01  I stopped using my wells Fargo account because...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (20347, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VntlVfEIBWPI"
   },
   "outputs": [],
   "source": [
    "def initialize_symspell():\n",
    "    sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "    dictionary_path = pkg_resources.resource_filename(\n",
    "        \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "    return sym_spell\n",
    "\n",
    "def preprocess_text(text, sym_spell):\n",
    "    try:\n",
    "        text = str(text).lower()\n",
    "        # Remove Wells Fargo mentions\n",
    "        text = re.sub(r'well?s?\\s*f[a-z]*go|wf\\b', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "        # Basic cleaning\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        # Spell correction\n",
    "        suggestions = sym_spell.lookup_compound(text, max_edit_distance=2)\n",
    "        if suggestions:\n",
    "            text = suggestions[0].term\n",
    "\n",
    "   # Tokenization and stop word removal - decided against lemmatization since it can affect word intensity\n",
    "        tokens = word_tokenize(text)\n",
    "        return ' '.join(token for token in tokens if token not in stopwords.words('english'))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def calculate_sentiment_scores(text):\n",
    "    blob = TextBlob(text)\n",
    "    vader_scores = SentimentIntensityAnalyzer().polarity_scores(text)\n",
    "\n",
    "    return {\n",
    "        'textblob_polarity': blob.sentiment.polarity,\n",
    "        'textblob_subjectivity': blob.sentiment.subjectivity,\n",
    "        **vader_scores\n",
    "    }\n",
    "\n",
    "def analyze_complaints(input_df, text_column='Complaint'):\n",
    "    df = input_df[input_df[text_column].notna() & (input_df[text_column].str.strip() != '')].copy()\n",
    "    # Sort by date index\n",
    "    df.sort_index(inplace=True)\n",
    "    print(\"Preprocessing text...\")\n",
    "    df['cleaned_text'] = df[text_column].apply(lambda x: preprocess_text(x, initialize_symspell()))\n",
    "\n",
    "    print(\"Calculating sentiment scores...\")\n",
    "    sentiments = df['cleaned_text'].apply(calculate_sentiment_scores)\n",
    "\n",
    "    sentiment_columns = ['textblob_polarity', 'textblob_subjectivity', 'nltk_si_neg', 'nltk_si_neu', 'nltk_si_pos', 'nltk_si_compound']\n",
    "    for col in sentiment_columns:\n",
    "        df[col] = sentiments.apply(lambda x: x.get(col, 0))\n",
    "\n",
    "    df.to_csv('results/sentiment_scores_raw.csv')\n",
    "\n",
    "    agg_metrics = {\n",
    "        'textblob_polarity': ['mean', 'std'],\n",
    "        'textblob_subjectivity': ['mean', 'std'],\n",
    "        'nltk_si_compound': ['mean', 'std'],\n",
    "        'nltk_si_pos': 'mean',\n",
    "        'nltk_si_neg': 'mean',\n",
    "        'nltk_si_neu': 'mean',\n",
    "        text_column: 'count'\n",
    "    }\n",
    "\n",
    "    daily_agg = df.groupby(df.index.date).agg(agg_metrics).round(4)\n",
    "    daily_agg.to_csv('results/sentiment_scores_daily.csv')\n",
    "\n",
    "    monthly_agg = df.groupby(df.index.to_period('M')).agg(agg_metrics).round(4)\n",
    "    monthly_agg.to_csv('results/sentiment_scores_monthly.csv')\n",
    "\n",
    "    return df, daily_agg, monthly_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUyTnNOCH2on",
    "outputId": "c208838f-9a81-4d28-cf78-758f506895b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text...\n"
     ]
    }
   ],
   "source": [
    "raw_scores, daily_sentiment, monthly_sentiment = analyze_complaints(df)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
